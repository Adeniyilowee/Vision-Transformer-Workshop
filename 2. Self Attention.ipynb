{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b33bd7c-bca7-4c00-932a-2921e73bffe3",
   "metadata": {},
   "source": [
    "<img src=\"images/00-image.png\" alt=\"encoder\" class=\"bg-primary\" width=\"100%\">\n",
    "\n",
    "\n",
    "[Image Reference](https://www.planetware.com/tourist-attractions-/potsdam-d-br-pt.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18cc9e8-6f36-4a06-b602-180e5bb95cbd",
   "metadata": {},
   "source": [
    "<h1><center> Attention Explained <center></h1>\n",
    "\n",
    "Vision Transformer (ViT) paper: [Paper Reference](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65045704-07d8-4ac0-8926-d28c303ffe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbaadedd-950a-46f3-af78-e6ea0e416c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da93f65-d75e-406d-92aa-00f3325c8bfc",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d063c7f2-7bde-4e1c-a8e7-d3740be445fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to find a way to parameterize each token so that we can rank them based on thier importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3064a66a-2b97-41d7-b0c3-be3a91b006ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704a078e-071f-4a89-9403-96bc396c09b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 0.9007, -2.1055],\n",
       "        [ 0.6784, -1.2345],\n",
       "        [-0.0431, -1.6047],\n",
       "        [-0.7521,  1.6487],\n",
       "        [-0.3925, -1.4036],\n",
       "        [-0.7279, -0.5594],\n",
       "        [-0.7688,  0.7624]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f0852-7eb1-4f7b-84d9-832b17cb67e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Version 1 : Using simple mathematical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5353dddf-4e31-4f6e-9a70-14d8884f485a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention1 = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_previous = x[b, :t+1]\n",
    "        attention1[b, t] = torch.mean(x_previous, 0) # also called bow: bag of words / pixel or bop in out case\n",
    "\n",
    "# bow is an averaging schemes, thus used as an attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33be8d65-1883-4aff-972f-b90ae729924d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 0.9007, -2.1055]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :1+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13a4f61-abfc-4382-b486-54ff33853063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4138, -0.3091])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2320e75d-0f7a-4adf-8cbf-8eeff52f5075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 0.9007, -2.1055],\n",
       "        [ 0.6784, -1.2345],\n",
       "        [-0.0431, -1.6047],\n",
       "        [-0.7521,  1.6487],\n",
       "        [-0.3925, -1.4036],\n",
       "        [-0.7279, -0.5594],\n",
       "        [-0.7688,  0.7624]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728a2fd9-b919-46a1-a18e-3b6bacde360c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176],\n",
       "        [ 0.8657, -0.8644],\n",
       "        [ 0.5422, -0.3617],\n",
       "        [ 0.3864, -0.5354],\n",
       "        [ 0.2272, -0.5388],\n",
       "        [ 0.1027, -0.3762]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce572b-d528-4868-99d7-553e7a46f8b0",
   "metadata": {},
   "source": [
    "### Replicating above by combining Matmul and Trill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b93810-eff2-438b-a2ad-167e1e5f76d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Explaining concept of matmul - example\n",
    "a = torch.ones(3,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d7caf1-3cbe-4358-9895-b7a9fc577dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [3., 0.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e29dfc48-2189-4524-9910-ad88e8fa8530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 2.],\n",
      "        [4., 2.],\n",
      "        [4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "c = a@b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed1d69c5-6a4e-4c28-aca8-748b8112c0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explaining concept of Tril - example\n",
    "torch.tril(torch.ones(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b6ed27f-e2c3-4b89-bbd8-d83eb3e5abc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.],\n",
      "        [0., 4.],\n",
      "        [0., 3.],\n",
      "        [8., 4.],\n",
      "        [0., 4.],\n",
      "        [1., 2.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.],\n",
      "        [14., 20.],\n",
      "        [14., 23.],\n",
      "        [22., 27.],\n",
      "        [22., 31.],\n",
      "        [23., 33.]])\n"
     ]
    }
   ],
   "source": [
    "# Combining tril and matmul\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(8, 8))\n",
    "b = torch.randint(0, 10, (8, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b157b2f4-eff0-4c40-8882-a93194ff44f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.],\n",
      "        [0., 4.],\n",
      "        [0., 3.],\n",
      "        [8., 4.],\n",
      "        [0., 4.],\n",
      "        [1., 2.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333],\n",
      "        [3.5000, 5.0000],\n",
      "        [2.8000, 4.6000],\n",
      "        [3.6667, 4.5000],\n",
      "        [3.1429, 4.4286],\n",
      "        [2.8750, 4.1250]])\n"
     ]
    }
   ],
   "source": [
    "# average and using tril : basically make everything we multiply with it average themselves\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(8,8))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (8, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c234214-7288-45c4-92d1-f8c6e111f46d",
   "metadata": {},
   "source": [
    "### Introducing and Establishing the SCORES / WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5131236-9f55-48b5-98ec-8be9aea2ed6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now appling it\n",
    "tril = torch.tril(torch.ones(T,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92a01e08-b003-46ca-9cb3-6e697efe27a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5384d71c-e6ef-4261-b7e9-65a67d5da55c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = tril / torch.sum(tril, 1, keepdim=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa3489af-753e-4a7f-a37c-267ee5ef924f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention2 = scores @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5b4aa47-09d5-4d80-9f19-22c651ef1078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attention2[0], attention1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18d9d6b6-14d2-4558-917d-1ce26bab2a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176],\n",
       "        [ 0.8657, -0.8644],\n",
       "        [ 0.5422, -0.3617],\n",
       "        [ 0.3864, -0.5354],\n",
       "        [ 0.2272, -0.5388],\n",
       "        [ 0.1027, -0.3762]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30f79e82-2bb9-46c1-8fd2-fe257c34058b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176],\n",
       "        [ 0.8657, -0.8644],\n",
       "        [ 0.5422, -0.3617],\n",
       "        [ 0.3864, -0.5354],\n",
       "        [ 0.2272, -0.5388],\n",
       "        [ 0.1027, -0.3762]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb78f3-b367-48d5-965f-f78db535e7af",
   "metadata": {},
   "source": [
    "- Now we have found a way to parameterize them such that we have our weight (average) separated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da4eb3-1f1a-408c-86b2-8b14ac8a16d1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec147993-65e9-492f-ac1d-d716982fca4d",
   "metadata": {},
   "source": [
    "## Version 2 : Using softmax - another method but better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8651e-b06c-4806-a614-9510980b8be2",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The weight is designed differently using softmax\n",
    "- Though here softmax would do the same thing as above by evenly distributing the weight aggregate\n",
    "- However, that is the case where we use 0 and 1, hence when we use weight that is data dependent (QKV in softmax), softmax would be better then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8121b3ec-a5ed-4ae9-8d03-b6163863cdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T)) # because 8 tokens must result into 8X8\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fe5a250-9c5f-4b1a-9568-7028afa73f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = torch.zeros(T,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2af0f658-54f9-4dd9-879d-77dcdafb422b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = scores.masked_fill(tril == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67619d42-6a0c-442f-96d7-d2e8ecab7c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6b466c5-debd-4296-990b-1a4db0262365",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_attn = torch.softmax(scores, dim=-1)\n",
    "p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d8a6dac-48d3-4949-90db-21f0685a258e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention3 = p_attn @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4c2b658-c419-43a6-8c7d-bb39f37da430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176],\n",
       "        [ 0.8657, -0.8644],\n",
       "        [ 0.5422, -0.3617],\n",
       "        [ 0.3864, -0.5354],\n",
       "        [ 0.2272, -0.5388],\n",
       "        [ 0.1027, -0.3762]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22f9b96b-5c6e-487e-afbf-4c59e7a16bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attention2[0], attention3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30b0c2-dd05-4164-8a66-9867b4d232ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6816930-ce86-4a6d-8fcc-7a357ad152a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Version 3: Final with self attention (Putting everything together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f4d72-339f-4768-9b37-346f42a53e97",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Here, the weight will be data dependent, thus making softmax very useful \n",
    "- The idea is, unlIke previous version, we do not want the values to be context average, otherwise it suggests that all token are equally important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c4ca8-4805-47be-abc9-54f086ebe620",
   "metadata": {},
   "source": [
    "Note below: In terms of image, we are no longer talking about 2D image, thus, arrangement of Channel, Width, Height no longer matter, everything is now 1D. Hence,\n",
    "- B = batch, in image = channel\n",
    "- T = time/Sentence, in image = flattend R/G/B \n",
    "- C = Depth dimension, can be any value you wish e.g RGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a5447ab-2628-45d1-8059-d7aa6d621056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2c84d-6107-4488-87f4-931532dec426",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Note: the QKV is for the weight initialization, hence must come out in block_size/context lenght size\n",
    "- The K and Q are the same values, but by using the transpose, each and every token can multiply its Key to all Query of the others\n",
    "- Then we can estimate its affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34f953ee-8ec0-4403-b23c-d9406b8c04fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_size = 3 # 16 # this is the \n",
    "key = nn.Linear(in_features=C, out_features=head_size)\n",
    "query = nn.Linear(in_features=C, out_features=head_size)\n",
    "value = nn.Linear(in_features=C, out_features=head_size)\n",
    "k = key(x) # shape = (B, T, 3)\n",
    "q = query(x) # shape = (B, T, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4061e0d3-4237-4941-8f17-c9d220552f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a88a6de4-3ecb-439e-9f51-c0ca8260a4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2410e3ef-5c12-40e4-9836-3600f299708a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matmul_qk = q@k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f6de4eb-46f4-4977-8233-979be4223233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_qk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa4da112-2984-450a-804b-65178764843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = matmul_qk * k.size(-1)**-0.5 # Scale Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29cc7045-6a41-4570-aa44-937f943771f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1586, -0.3868, -0.5070, -0.2084, -0.0627, -0.2190, -0.2884,\n",
       "          -0.3706],\n",
       "         [-0.0637, -0.2880, -0.2969, -0.0997, -0.0577, -0.1326, -0.2706,\n",
       "          -0.2463],\n",
       "         [-0.1438, -0.2684, -0.2761, -0.1715, -0.1470, -0.1879, -0.2572,\n",
       "          -0.2476],\n",
       "         [-0.0794, -0.3266, -0.4379, -0.4069, -0.3157, -0.3716, -0.2495,\n",
       "          -0.3684],\n",
       "         [-0.0062, -0.3520, -0.4828, -0.3935, -0.2770, -0.3618, -0.2584,\n",
       "          -0.3889],\n",
       "         [-0.0617, -0.3231, -0.4159, -0.3372, -0.2518, -0.3175, -0.2558,\n",
       "          -0.3457],\n",
       "         [-0.0057, -0.3045, -0.3203, -0.0646, -0.0069, -0.1058, -0.2793,\n",
       "          -0.2523],\n",
       "         [-0.0840, -0.2928, -0.3252, -0.1857, -0.1360, -0.1994, -0.2629,\n",
       "          -0.2748]],\n",
       "\n",
       "        [[-0.6574,  0.0109, -0.2844, -0.3501, -0.3486, -0.5219, -0.0957,\n",
       "          -0.5622],\n",
       "         [-0.2279, -0.3581, -0.1941, -0.1711, -0.3596, -0.2271, -0.4246,\n",
       "          -0.2480],\n",
       "         [-0.5419, -0.2499, -0.1904, -0.2011, -0.5337, -0.4346, -0.4511,\n",
       "          -0.5031],\n",
       "         [-0.5961, -0.2189, -0.1952, -0.2140, -0.5498, -0.4710, -0.4352,\n",
       "          -0.5451],\n",
       "         [-0.3754, -0.1227, -0.2719, -0.3004, -0.2334, -0.3338, -0.1322,\n",
       "          -0.3389],\n",
       "         [-0.6020, -0.0779, -0.2550, -0.3013, -0.3966, -0.4818, -0.2063,\n",
       "          -0.5282],\n",
       "         [-0.1945, -0.2542, -0.2442, -0.2399, -0.2111, -0.2108, -0.2313,\n",
       "          -0.2028],\n",
       "         [-0.5944, -0.0409, -0.2722, -0.3253, -0.3476, -0.4788, -0.1401,\n",
       "          -0.5157]],\n",
       "\n",
       "        [[-0.5020, -0.1737, -0.1840, -0.4949, -0.4952, -0.1785, -0.0301,\n",
       "          -0.2794],\n",
       "         [-0.3459, -0.2390, -0.3228, -0.3318, -0.4204, -0.2597, -0.2840,\n",
       "          -0.4153],\n",
       "         [-0.2891, -0.2430, -0.2886, -0.2816, -0.3302, -0.2541, -0.2732,\n",
       "          -0.3357],\n",
       "         [-0.5077, -0.1743, -0.1914, -0.4996, -0.5072, -0.1807, -0.0360,\n",
       "          -0.2934],\n",
       "         [-0.4399, -0.1808, -0.1584, -0.4388, -0.4055, -0.1773, -0.0327,\n",
       "          -0.2105],\n",
       "         [-0.3335, -0.2395, -0.3137, -0.3210, -0.3995, -0.2578, -0.2796,\n",
       "          -0.3954],\n",
       "         [-0.2073, -0.2744, -0.3495, -0.1974, -0.2822, -0.2918, -0.3918,\n",
       "          -0.3889],\n",
       "         [-0.2873, -0.2285, -0.2255, -0.2867, -0.2814, -0.2282, -0.1973,\n",
       "          -0.2388]],\n",
       "\n",
       "        [[-0.1269,  0.0389, -0.1451, -0.5696, -0.3271, -0.2782, -0.0468,\n",
       "          -0.4572],\n",
       "         [-0.0848,  0.1319, -0.1161, -0.6669, -0.3517, -0.2877,  0.0107,\n",
       "          -0.5322],\n",
       "         [ 0.0318,  0.1733, -0.1797, -0.4302, -0.2712, -0.2293, -0.1402,\n",
       "          -0.6583],\n",
       "         [-0.1689, -0.1473, -0.2360, -0.2545, -0.2387, -0.2323, -0.2378,\n",
       "          -0.3468],\n",
       "         [-0.0675,  0.0241, -0.2013, -0.3653, -0.2616, -0.2345, -0.1750,\n",
       "          -0.5077],\n",
       "         [-0.0552,  0.0514, -0.1927, -0.3941, -0.2689, -0.2373, -0.1580,\n",
       "          -0.5297],\n",
       "         [ 0.0937,  0.2612, -0.1696, -0.4587, -0.2737, -0.2241, -0.1259,\n",
       "          -0.7501],\n",
       "         [ 0.1296,  0.1549, -0.2683, -0.1077, -0.1674, -0.1596, -0.3428,\n",
       "          -0.7438]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad6b22-652e-48a8-8076-6d93ed31958a",
   "metadata": {},
   "source": [
    "- The above has made our initial weight to be data driven\n",
    "- Now we need to truncate using trill below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa2a367d-1eb1-4c81-bc84-c8f40c5142c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1586,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0637, -0.2880,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.1438, -0.2684, -0.2761,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0794, -0.3266, -0.4379, -0.4069,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0062, -0.3520, -0.4828, -0.3935, -0.2770,    -inf,    -inf,    -inf],\n",
       "        [-0.0617, -0.3231, -0.4159, -0.3372, -0.2518, -0.3175,    -inf,    -inf],\n",
       "        [-0.0057, -0.3045, -0.3203, -0.0646, -0.0069, -0.1058, -0.2793,    -inf],\n",
       "        [-0.0840, -0.2928, -0.3252, -0.1857, -0.1360, -0.1994, -0.2629, -0.2748]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "scores = scores.masked_fill(tril == 0, float('-inf')) # Comment out to obtain bi-directional effect\n",
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf1c1de6-c0e0-424b-854b-081cd239e33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5558, 0.4442, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3625, 0.3200, 0.3176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3125, 0.2440, 0.2183, 0.2252, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2652, 0.1877, 0.1647, 0.1801, 0.2023, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2070, 0.1593, 0.1452, 0.1571, 0.1711, 0.1602, 0.0000, 0.0000],\n",
       "        [0.1645, 0.1220, 0.1201, 0.1551, 0.1643, 0.1488, 0.1251, 0.0000],\n",
       "        [0.1428, 0.1159, 0.1122, 0.1290, 0.1356, 0.1272, 0.1194, 0.1180]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_attn = torch.softmax(scores, dim=-1)\n",
    "p_attn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d8f662c-260b-4bc1-acef-26b87efbcf68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa99856f-a585-4180-a078-2ff4bf8c7b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x) # we aggregate the values not the exact token, it is also learnable\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29f8dc61-ff33-42b9-b2ab-6e7579794263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention = p_attn @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebf1dba8-8b3d-489d-bd19-71bab3c381e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f1c84-3f10-4478-83d6-54dfeeae76b2",
   "metadata": {},
   "source": [
    "## Puthing all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "562ca334-70e9-41ca-8b6e-b37fa2539843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        #self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # input of size (batch, latent-space, feature map)\n",
    "        # output of size (batch, latent-space, head size)\n",
    "        \n",
    "        B,T,C = key.shape\n",
    "        \n",
    "        key = self.key(key)   # (B,T,hs)\n",
    "        query = self.query(query) # (B,T,hs)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        matmul_qk = query @ key.transpose(-2,-1) # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        \n",
    "        scores = matmul_qk * key.size(-1)**-0.5 # Scale Factor\n",
    "\n",
    "        scores = scores.masked_fill(torch.tril(torch.ones(T,T)) == 0, float('-inf'))# (B, T, T) # Comment out to obtain bi-directional effect\n",
    "        \n",
    "        p_attn = F.softmax(scores, dim=-1) # (B, T, T)\n",
    "        \n",
    "        p_attn = self.dropout(p_attn)\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        value = self.value(value) # (B,T,hs)\n",
    "        attention = p_attn @ value # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        \n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846e14d-2452-444c-b112-19165c2fc4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
