{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaadedd-950a-46f3-af78-e6ea0e416c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da93f65-d75e-406d-92aa-00f3325c8bfc",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d063c7f2-7bde-4e1c-a8e7-d3740be445fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to find a way to parameterize each token so that we can rank them based on thier importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3064a66a-2b97-41d7-b0c3-be3a91b006ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704a078e-071f-4a89-9403-96bc396c09b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f0852-7eb1-4f7b-84d9-832b17cb67e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## v1 : No weight bcos everything is done together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5353dddf-4e31-4f6e-9a70-14d8884f485a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0) # mean in vertical direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33be8d65-1883-4aff-972f-b90ae729924d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :1+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13a4f61-abfc-4382-b486-54ff33853063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0894, -0.4926])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728a2fd9-b919-46a1-a18e-3b6bacde360c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43380ff3-3855-4a8d-8fd5-aeb2860addc7",
   "metadata": {},
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce572b-d528-4868-99d7-553e7a46f8b0",
   "metadata": {},
   "source": [
    "## v2 : Replicating above by combining Matmul and Trill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98f78992-592e-40e6-8039-c69e4f1bfe76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Explaining concept of matmul - example\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1d69c5-6a4e-4c28-aca8-748b8112c0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explaining concept of Tril - example\n",
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b6ed27f-e2c3-4b89-bbd8-d83eb3e5abc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Combining tril and matmul\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b157b2f4-eff0-4c40-8882-a93194ff44f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# average and using tril : basically make everything we multiply with it average themselves\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c234214-7288-45c4-92d1-f8c6e111f46d",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5131236-9f55-48b5-98ec-8be9aea2ed6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now appling it\n",
    "tril = torch.tril(torch.ones(T,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92a01e08-b003-46ca-9cb3-6e697efe27a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5384d71c-e6ef-4261-b7e9-65a67d5da55c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = tril / torch.sum(tril, 1, keepdim=True)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa3489af-753e-4a7f-a37c-267ee5ef924f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xbow2 = weight @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5b4aa47-09d5-4d80-9f19-22c651ef1078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow2[0],xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18d9d6b6-14d2-4558-917d-1ce26bab2a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8cd0163-5243-464c-95f1-b89e3ac56c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have found a way to parameterize them such that we have our weight separated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da4eb3-1f1a-408c-86b2-8b14ac8a16d1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec147993-65e9-492f-ac1d-d716982fca4d",
   "metadata": {},
   "source": [
    "## v3 : Using softmax - another method but better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "043a1953-dd58-4226-8577-af5acea01fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - the weight is designed differently using softmax\n",
    "# - Though in v3, softmax would do the same thing as the v2 by evenly distributing the weight aggregate\n",
    "# - However, that is the case where we use 0 and 1, hence when we use weight that is data dependent (QKV in softmax), softmax would be better then, see V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8121b3ec-a5ed-4ae9-8d03-b6163863cdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe5a250-9c5f-4b1a-9568-7028afa73f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = torch.zeros(T,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2af0f658-54f9-4dd9-879d-77dcdafb422b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = weight.masked_fill(tril == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67619d42-6a0c-442f-96d7-d2e8ecab7c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6b466c5-debd-4296-990b-1a4db0262365",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.softmax(weight, dim=-1)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d8a6dac-48d3-4949-90db-21f0685a258e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xbow3 = weight @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4c2b658-c419-43a6-8c7d-bb39f37da430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22f9b96b-5c6e-487e-afbf-4c59e7a16bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow2[0], xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30b0c2-dd05-4164-8a66-9867b4d232ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6816930-ce86-4a6d-8fcc-7a357ad152a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## V4 : Final with self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d28a69e-2dae-420f-8d9f-b79c39b83c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Here, the weight will be data dependent, thus making softmax very useful \n",
    "# - The idea is, unlke prev version, we do not want the values to be uniform, otherwise it suggsts that all token are equally important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69fb4bb1-9efd-4e9f-95ff-63293f4daccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note below: In terms of image, we are no longer talking about 2D image, thus, arrangement of Channel, Width, Height no longer matter, everything is now 1D\n",
    "# Hence B = batch, in image = channel\n",
    "#       T = time/Sentence, in image = flattend R/G/B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a5447ab-2628-45d1-8059-d7aa6d621056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10ef6ea9-3336-4e2c-9529-c182da6b4cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# self attention starts here --- single head\n",
    "# Note: the QKV thing is for the weight initialization, hence must come out in block_size/context lenght size\n",
    "# the K and Q are the same values, but by using the transpose, each and every token can multiply its Key to all Query of the others\n",
    "# then we can estimate its affinity\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # same as query\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # shape = (B, T, 16)\n",
    "q = query(x) # shape = (B, T, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4061e0d3-4237-4941-8f17-c9d220552f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a88a6de4-3ecb-439e-9f51-c0ca8260a4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2410e3ef-5c12-40e4-9836-3600f299708a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = q@k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f6de4eb-46f4-4977-8233-979be4223233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29cc7045-6a41-4570-aa44-937f943771f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.1839e-02, -1.2717e-01,  1.8555e-01,  2.8362e-01,  1.3338e-01,\n",
       "          -4.1065e-01,  6.4280e-02, -2.3709e-01],\n",
       "         [-2.7879e-02, -2.6872e-01, -1.4179e-02, -1.3808e-02,  3.4634e-01,\n",
       "          -1.0934e-01, -8.2021e-02,  5.0689e-01],\n",
       "         [ 1.6034e-01, -3.0773e-01,  5.5218e-01,  8.4200e-01,  3.0638e-01,\n",
       "          -1.1863e+00,  2.1087e-01, -8.2965e-01],\n",
       "         [ 2.4276e-01, -4.5681e-01,  8.3370e-01,  1.2711e+00,  4.5259e-01,\n",
       "          -1.7872e+00,  3.2054e-01, -1.2664e+00],\n",
       "         [ 1.0243e-02,  3.9295e-01, -6.9511e-02, -1.1600e-01, -4.9214e-01,\n",
       "           3.2836e-01,  7.1550e-02, -5.1735e-01],\n",
       "         [-3.1340e-01,  4.3864e-01, -1.0379e+00, -1.5788e+00, -3.9689e-01,\n",
       "           2.1592e+00, -4.3509e-01,  1.8049e+00],\n",
       "         [ 7.8257e-02, -2.3943e-01,  2.9217e-01,  4.4768e-01,  2.6021e-01,\n",
       "          -6.6641e-01,  9.0359e-02, -3.0451e-01],\n",
       "         [-3.4883e-01,  1.2351e+00, -1.3450e+00, -2.0646e+00, -1.3681e+00,\n",
       "           3.1349e+00, -3.7915e-01,  1.1684e+00]],\n",
       "\n",
       "        [[ 2.6985e+00,  6.1368e-01, -4.0637e+00,  3.0063e+00,  1.8685e-01,\n",
       "          -2.3401e+00,  8.6400e-01,  3.1605e+00],\n",
       "         [-8.5220e-02, -3.4023e-01,  3.7297e-02, -3.3524e-01,  4.9395e-01,\n",
       "           2.2065e-01,  2.1634e-01, -9.5986e-01],\n",
       "         [-4.2620e+00, -1.0603e+00,  6.3923e+00, -4.8163e+00, -1.5328e-01,\n",
       "           3.7376e+00, -1.2955e+00, -5.2357e+00],\n",
       "         [ 2.4829e+00,  3.2435e-01, -3.8072e+00,  2.5861e+00,  5.4628e-01,\n",
       "          -2.0432e+00,  9.7743e-01,  2.2639e+00],\n",
       "         [ 1.2757e+00,  7.8996e-01, -1.7792e+00,  1.7955e+00, -6.9038e-01,\n",
       "          -1.3348e+00,  2.8899e-02,  2.8339e+00],\n",
       "         [-2.0204e+00, -3.1274e-01,  3.0842e+00, -2.1410e+00, -3.6851e-01,\n",
       "           1.6850e+00, -7.5833e-01, -1.9730e+00],\n",
       "         [ 1.3947e+00,  5.6080e-01, -2.0311e+00,  1.7362e+00, -2.8297e-01,\n",
       "          -1.3209e+00,  2.6156e-01,  2.2865e+00],\n",
       "         [ 1.2871e+00, -5.6734e-01, -2.1823e+00,  7.8979e-01,  1.4290e+00,\n",
       "          -7.2280e-01,  1.0651e+00, -7.9790e-01]],\n",
       "\n",
       "        [[ 5.2067e-01, -8.1574e-01, -3.1660e-02,  9.1805e-01,  3.3537e-01,\n",
       "          -5.7621e-01,  1.0356e+00,  4.2871e-01],\n",
       "         [-9.0597e-01,  1.3917e+00,  1.6905e-01, -1.5827e+00, -6.6117e-01,\n",
       "           8.2202e-01, -1.6927e+00, -7.9554e-01],\n",
       "         [ 3.3932e-01, -4.1764e-01, -4.8914e-01,  5.3770e-01,  5.3770e-01,\n",
       "           3.6693e-01,  2.2610e-01,  4.8324e-01],\n",
       "         [ 9.6601e-01, -1.4987e+00, -1.1931e-01,  1.6954e+00,  6.6348e-01,\n",
       "          -9.7306e-01,  1.8633e+00,  8.2175e-01],\n",
       "         [ 8.2663e-02, -2.0714e-01,  3.1412e-01,  1.8702e-01, -1.6415e-01,\n",
       "          -5.9722e-01,  4.7009e-01, -7.0801e-02],\n",
       "         [-1.1641e+00,  1.6432e+00,  8.1322e-01, -1.9565e+00, -1.2555e+00,\n",
       "           1.1173e-01, -1.6041e+00, -1.2815e+00],\n",
       "         [ 1.3909e+00, -2.0699e+00, -5.3333e-01,  2.3944e+00,  1.2016e+00,\n",
       "          -8.2811e-01,  2.3365e+00,  1.3405e+00],\n",
       "         [ 2.6729e-01, -4.6835e-01,  1.8760e-01,  4.9765e-01,  3.3299e-02,\n",
       "          -6.1885e-01,  7.2687e-01,  1.3138e-01]],\n",
       "\n",
       "        [[ 4.3596e+00,  3.6813e+00, -3.6430e+00,  7.2153e+00,  3.6348e+00,\n",
       "           2.2253e+00,  2.5669e+00,  4.3654e+00],\n",
       "         [ 3.3525e+00,  2.7869e+00, -2.9581e+00,  5.5941e+00,  2.8870e+00,\n",
       "           1.6896e+00,  1.7547e+00,  3.3131e+00],\n",
       "         [-4.8152e+00, -4.2227e+00,  3.4650e+00, -7.8066e+00, -3.6871e+00,\n",
       "          -2.5351e+00, -3.6168e+00, -4.9777e+00],\n",
       "         [ 7.5567e+00,  6.4265e+00, -6.1518e+00,  1.2459e+01,  6.2050e+00,\n",
       "           3.8797e+00,  4.6770e+00,  7.6121e+00],\n",
       "         [ 4.3222e+00,  3.7416e+00, -3.2841e+00,  7.0580e+00,  3.4115e+00,\n",
       "           2.2515e+00,  3.0033e+00,  4.4195e+00],\n",
       "         [ 2.0631e+00,  1.7204e+00, -1.8013e+00,  3.4371e+00,  1.7655e+00,\n",
       "           1.0424e+00,  1.1066e+00,  2.0442e+00],\n",
       "         [ 9.2684e-01,  5.6340e-01, -1.5562e+00,  1.7616e+00,  1.2311e+00,\n",
       "           3.6496e-01, -5.4794e-01,  7.0954e-01],\n",
       "         [ 4.0377e+00,  3.3656e+00, -3.5301e+00,  6.7279e+00,  3.4580e+00,\n",
       "           2.0394e+00,  2.1588e+00,  3.9993e+00]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "590b7ea5-091f-4b00-b494-224e5f3f565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above has made our initial weight to be data driven\n",
    "# Now we need to truncate using trill below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa2a367d-1eb1-4c81-bc84-c8f40c5142c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0518,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0279, -0.2687,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1603, -0.3077,  0.5522,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.2428, -0.4568,  0.8337,  1.2711,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0102,  0.3929, -0.0695, -0.1160, -0.4921,    -inf,    -inf,    -inf],\n",
       "        [-0.3134,  0.4386, -1.0379, -1.5788, -0.3969,  2.1592,    -inf,    -inf],\n",
       "        [ 0.0783, -0.2394,  0.2922,  0.4477,  0.2602, -0.6664,  0.0904,    -inf],\n",
       "        [-0.3488,  1.2351, -1.3450, -2.0646, -1.3681,  3.1349, -0.3791,  1.1684]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "#weight = torch.zeros(T,T)\n",
    "weight = weight.masked_fill(tril == 0, float('-inf'))\n",
    "weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf1c1de6-c0e0-424b-854b-081cd239e33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5599, 0.4401, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3220, 0.2016, 0.4764, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1640, 0.0815, 0.2961, 0.4585, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2051, 0.3007, 0.1894, 0.1808, 0.1241, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0600, 0.1273, 0.0291, 0.0169, 0.0552, 0.7114, 0.0000, 0.0000],\n",
       "        [0.1408, 0.1025, 0.1744, 0.2038, 0.1690, 0.0669, 0.1426, 0.0000],\n",
       "        [0.0223, 0.1086, 0.0082, 0.0040, 0.0080, 0.7257, 0.0216, 0.1016]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.softmax(weight, dim=-1)\n",
    "weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d8f662c-260b-4bc1-acef-26b87efbcf68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa99856f-a585-4180-a078-2ff4bf8c7b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x) # we aggregate the values not the exact token, it is also learnable\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29f8dc61-ff33-42b9-b2ab-6e7579794263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xbow3 = weight @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1dba8-8b3d-489d-bd19-71bab3c381e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b58f1c84-3f10-4478-83d6-54dfeeae76b2",
   "metadata": {},
   "source": [
    "## Puthing all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "562ca334-70e9-41ca-8b6e-b37fa2539843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        #self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # input of size (batch, latent-space, feature map)\n",
    "        # output of size (batch, latent-space, head size)\n",
    "        \n",
    "        B,T,C = key.shape\n",
    "        \n",
    "        key = self.key(key)   # (B,T,hs)\n",
    "        query = self.query(query) # (B,T,hs)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = query @ key.transpose(-2,-1) # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        \n",
    "        wei = wei * key.shape[-1]**-0.5 # Scale Factor\n",
    "\n",
    "        wei = wei.masked_fill(torch.tril(torch.ones(T,T)) == 0, float('-inf'))# (B, T, T)\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        \n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        value = self.value(value) # (B,T,hs)\n",
    "        out = wei @ value # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846e14d-2452-444c-b112-19165c2fc4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
