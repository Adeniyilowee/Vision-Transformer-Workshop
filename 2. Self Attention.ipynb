{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b33bd7c-bca7-4c00-932a-2921e73bffe3",
   "metadata": {},
   "source": [
    "<img src=\"images/00-image.png\" alt=\"encoder\" class=\"bg-primary\" width=\"100%\">\n",
    "\n",
    "\n",
    "[Image Reference](https://www.planetware.com/tourist-attractions-/potsdam-d-br-pt.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18cc9e8-6f36-4a06-b602-180e5bb95cbd",
   "metadata": {},
   "source": [
    "<h1><center> Attention Explained <center></h1>\n",
    "\n",
    "Vision Transformer (ViT) paper: [Paper Reference](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaadedd-950a-46f3-af78-e6ea0e416c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da93f65-d75e-406d-92aa-00f3325c8bfc",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d063c7f2-7bde-4e1c-a8e7-d3740be445fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to find a way to parameterize each token so that we can rank them based on thier importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3064a66a-2b97-41d7-b0c3-be3a91b006ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704a078e-071f-4a89-9403-96bc396c09b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f0852-7eb1-4f7b-84d9-832b17cb67e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Version 1 : Using simple mathematical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5353dddf-4e31-4f6e-9a70-14d8884f485a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_bop = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_previous = x[b, :t+1]\n",
    "        x_bop[b, t] = torch.mean(x_previous, 0) # bop: bag of pixel (average schemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33be8d65-1883-4aff-972f-b90ae729924d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :1+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d13a4f61-abfc-4382-b486-54ff33853063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0894, -0.4926])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bop[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2320e75d-0f7a-4adf-8cbf-8eeff52f5075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "728a2fd9-b919-46a1-a18e-3b6bacde360c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bop[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce572b-d528-4868-99d7-553e7a46f8b0",
   "metadata": {},
   "source": [
    "### Replicating above by combining Matmul and Trill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7b93810-eff2-438b-a2ad-167e1e5f76d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Explaining concept of matmul - example\n",
    "a = torch.ones(3,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93d7caf1-3cbe-4358-9895-b7a9fc577dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8., 6.],\n",
      "        [5., 2.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e29dfc48-2189-4524-9910-ad88e8fa8530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17., 12.],\n",
      "        [17., 12.],\n",
      "        [17., 12.]])\n"
     ]
    }
   ],
   "source": [
    "c = a@b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed1d69c5-6a4e-4c28-aca8-748b8112c0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explaining concept of Tril - example\n",
    "torch.tril(torch.ones(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b6ed27f-e2c3-4b89-bbd8-d83eb3e5abc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.],\n",
      "        [0., 4.],\n",
      "        [0., 3.],\n",
      "        [8., 4.],\n",
      "        [0., 4.],\n",
      "        [1., 2.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.],\n",
      "        [14., 20.],\n",
      "        [14., 23.],\n",
      "        [22., 27.],\n",
      "        [22., 31.],\n",
      "        [23., 33.]])\n"
     ]
    }
   ],
   "source": [
    "# Combining tril and matmul\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(8, 8))\n",
    "b = torch.randint(0, 10, (8, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b157b2f4-eff0-4c40-8882-a93194ff44f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.],\n",
      "        [0., 4.],\n",
      "        [0., 3.],\n",
      "        [8., 4.],\n",
      "        [0., 4.],\n",
      "        [1., 2.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333],\n",
      "        [3.5000, 5.0000],\n",
      "        [2.8000, 4.6000],\n",
      "        [3.6667, 4.5000],\n",
      "        [3.1429, 4.4286],\n",
      "        [2.8750, 4.1250]])\n"
     ]
    }
   ],
   "source": [
    "# average and using tril : basically make everything we multiply with it average themselves\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(8,8))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (8, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c234214-7288-45c4-92d1-f8c6e111f46d",
   "metadata": {},
   "source": [
    "### Introducing and Establishing the WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5131236-9f55-48b5-98ec-8be9aea2ed6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now appling it\n",
    "tril = torch.tril(torch.ones(T,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92a01e08-b003-46ca-9cb3-6e697efe27a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5384d71c-e6ef-4261-b7e9-65a67d5da55c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = tril / torch.sum(tril, 1, keepdim=True)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa3489af-753e-4a7f-a37c-267ee5ef924f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_bop2 = weight @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5b4aa47-09d5-4d80-9f19-22c651ef1078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x_bop2[0], x_bop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18d9d6b6-14d2-4558-917d-1ce26bab2a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30f79e82-2bb9-46c1-8fd2-fe257c34058b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bop2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb78f3-b367-48d5-965f-f78db535e7af",
   "metadata": {},
   "source": [
    "- Now we have found a way to parameterize them such that we have our weight (average) separated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da4eb3-1f1a-408c-86b2-8b14ac8a16d1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec147993-65e9-492f-ac1d-d716982fca4d",
   "metadata": {},
   "source": [
    "## Version 2 : Using softmax - another method but better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8651e-b06c-4806-a614-9510980b8be2",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The weight is designed differently using softmax\n",
    "- Though here softmax would do the same thing as above by evenly distributing the weight aggregate\n",
    "- However, that is the case where we use 0 and 1, hence when we use weight that is data dependent (QKV in softmax), softmax would be better then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8121b3ec-a5ed-4ae9-8d03-b6163863cdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T)) # because 8 tokens must result into 8X8\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fe5a250-9c5f-4b1a-9568-7028afa73f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = torch.zeros(T,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2af0f658-54f9-4dd9-879d-77dcdafb422b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = weight.masked_fill(tril == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67619d42-6a0c-442f-96d7-d2e8ecab7c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6b466c5-debd-4296-990b-1a4db0262365",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.softmax(weight, dim=-1)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d8a6dac-48d3-4949-90db-21f0685a258e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_bop3 = weight @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4c2b658-c419-43a6-8c7d-bb39f37da430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bop3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22f9b96b-5c6e-487e-afbf-4c59e7a16bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x_bop2[0], x_bop3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30b0c2-dd05-4164-8a66-9867b4d232ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6816930-ce86-4a6d-8fcc-7a357ad152a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Version 3: Final with self attention (Putting everything together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f4d72-339f-4768-9b37-346f42a53e97",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Here, the weight will be data dependent, thus making softmax very useful \n",
    "- The idea is, unlIke previous version, we do not want the values to be context average, otherwise it suggests that all token are equally important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c4ca8-4805-47be-abc9-54f086ebe620",
   "metadata": {},
   "source": [
    "Note below: In terms of image, we are no longer talking about 2D image, thus, arrangement of Channel, Width, Height no longer matter, everything is now 1D. Hence,\n",
    "- B = batch, in image = channel\n",
    "- T = time/Sentence, in image = flattend R/G/B \n",
    "- C = Depth dimension, can be any value you wish e.g RGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a5447ab-2628-45d1-8059-d7aa6d621056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2c84d-6107-4488-87f4-931532dec426",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Note: the QKV is for the weight initialization, hence must come out in block_size/context lenght size\n",
    "- The K and Q are the same values, but by using the transpose, each and every token can multiply its Key to all Query of the others\n",
    "- Then we can estimate its affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "34f953ee-8ec0-4403-b23c-d9406b8c04fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_size = 3 # 16 # this is the \n",
    "key = nn.Linear(in_features=C, out_features=head_size, bias=False)\n",
    "query = nn.Linear(in_features=C, out_features=head_size, bias=False)\n",
    "value = nn.Linear(in_features=C, out_features=head_size, bias=False)\n",
    "k = key(x) # shape = (B, T, 3)\n",
    "q = query(x) # shape = (B, T, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4061e0d3-4237-4941-8f17-c9d220552f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a88a6de4-3ecb-439e-9f51-c0ca8260a4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2410e3ef-5c12-40e4-9836-3600f299708a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = q@k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f6de4eb-46f4-4977-8233-979be4223233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "29cc7045-6a41-4570-aa44-937f943771f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8611e-01,  1.1800e-01,  1.5668e-01,  1.2040e-01, -4.4128e-02,\n",
       "          -1.7572e-01,  6.6347e-02,  1.3252e-02],\n",
       "         [ 2.1909e-01,  6.5659e-02,  3.5945e-01,  1.8460e-01, -1.3056e-01,\n",
       "          -2.4246e-01,  6.7986e-02, -6.7991e-02],\n",
       "         [-8.4862e-02,  1.2121e-01, -4.8961e-01, -1.5732e-01,  2.0796e-01,\n",
       "           1.6520e-01, -6.0769e-03,  1.9369e-01],\n",
       "         [ 6.1241e-02,  8.1695e-02, -5.0868e-02,  1.4532e-02,  3.1487e-02,\n",
       "          -3.6983e-02,  2.7754e-02,  5.3281e-02],\n",
       "         [ 6.4368e-02, -3.7803e-02,  2.4202e-01,  8.7650e-02, -9.9636e-02,\n",
       "          -9.8991e-02,  1.2087e-02, -8.5132e-02],\n",
       "         [-1.2658e-01, -1.1586e-01, -2.1478e-02, -6.1049e-02, -8.2046e-03,\n",
       "           1.0220e-01, -5.0043e-02, -4.9649e-02],\n",
       "         [ 8.0312e-02,  4.0801e-02,  9.1786e-02,  5.7878e-02, -2.9902e-02,\n",
       "          -8.0747e-02,  2.7233e-02, -5.8286e-03],\n",
       "         [ 1.2862e-01, -2.0452e-03,  3.0800e-01,  1.3213e-01, -1.2021e-01,\n",
       "          -1.6207e-01,  3.4304e-02, -8.6237e-02]],\n",
       "\n",
       "        [[ 2.2267e-02,  4.7600e-02,  3.1935e-02, -3.1695e-03, -1.1417e-02,\n",
       "          -9.1772e-02, -4.1823e-02, -6.9742e-03],\n",
       "         [ 7.1406e-02,  1.1870e-01,  7.3775e-02, -8.9060e-05, -1.9808e-01,\n",
       "          -3.7444e-01, -1.5487e-01, -4.7951e-03],\n",
       "         [ 5.2015e-02,  8.2558e-02,  5.0446e-02,  1.0943e-03, -1.6287e-01,\n",
       "          -2.8198e-01, -1.1520e-01, -1.4714e-03],\n",
       "         [-1.0234e-02, -1.1802e-02, -6.1794e-03, -1.5331e-03,  5.3163e-02,\n",
       "           6.5962e-02,  2.5379e-02, -2.0087e-03],\n",
       "         [ 1.0180e-01,  5.6155e-02,  9.8072e-03,  3.3425e-02, -8.2011e-01,\n",
       "          -8.0072e-01, -2.8988e-01,  5.1677e-02],\n",
       "         [-3.5578e-02, -1.5620e-01, -1.1863e-01,  2.8847e-02, -3.6291e-01,\n",
       "          -4.2548e-02,  1.7850e-02,  5.2619e-02],\n",
       "         [-2.7276e-02, -7.9055e-02, -5.6620e-02,  1.0039e-02, -8.4686e-02,\n",
       "           6.3442e-02,  3.8553e-02,  1.9280e-02],\n",
       "         [-1.9294e-02, -2.3676e-02, -1.2852e-02, -2.4677e-03,  9.3455e-02,\n",
       "           1.2100e-01,  4.6977e-02, -3.0498e-03]],\n",
       "\n",
       "        [[-1.3770e-01,  1.0402e-02, -6.7895e-02,  1.0336e-02, -2.0981e-01,\n",
       "           5.8305e-02, -7.9425e-02, -3.8755e-02],\n",
       "         [ 1.2515e-02, -9.0219e-04,  7.3446e-03,  3.2300e-04,  1.6590e-02,\n",
       "          -7.0629e-03,  5.3141e-03,  1.9193e-03],\n",
       "         [-1.0578e-02,  1.9726e-03,  2.6612e-02,  3.5035e-02, -8.3363e-02,\n",
       "          -4.3350e-02, -5.7766e-02, -4.6458e-02],\n",
       "         [ 7.1999e-02, -4.1766e-03,  6.9742e-02,  3.1432e-02,  3.7360e-02,\n",
       "          -8.1942e-02, -1.4052e-02, -2.6513e-02],\n",
       "         [-3.3091e-01,  2.2519e-02, -2.3041e-01, -4.7505e-02, -3.6212e-01,\n",
       "           2.4117e-01, -8.1713e-02, -1.2668e-03],\n",
       "         [-2.7829e-02,  3.3879e-04, -6.1550e-02, -4.9366e-02,  5.8652e-02,\n",
       "           8.3659e-02,  6.1588e-02,  5.7509e-02],\n",
       "         [-1.7247e-01,  1.1124e-02, -1.3670e-01, -4.2636e-02, -1.5363e-01,\n",
       "           1.5067e-01, -1.5615e-02,  2.2041e-02],\n",
       "         [-1.1706e-01,  7.2399e-03, -1.0120e-01, -3.7991e-02, -8.6493e-02,\n",
       "           1.1491e-01,  3.0607e-03,  2.6455e-02]],\n",
       "\n",
       "        [[-3.6404e-01,  2.3184e-01,  1.0647e-01, -5.0717e-03,  3.2770e-01,\n",
       "          -1.7495e-01,  8.1801e-02, -7.7884e-02],\n",
       "         [ 4.7316e-01, -8.7930e-02,  1.3707e-02,  4.6332e-02, -5.6369e-01,\n",
       "           1.5885e-01, -7.3636e-03, -1.6339e-01],\n",
       "         [ 2.7847e-01, -2.5247e-02,  2.6956e-02,  3.2203e-02, -3.4886e-01,\n",
       "           8.4974e-02,  7.9560e-03, -1.2902e-01],\n",
       "         [ 3.9867e-02,  1.4350e-02,  1.6663e-02,  7.9558e-03, -6.1541e-02,\n",
       "           6.3947e-03,  9.4696e-03, -4.0747e-02],\n",
       "         [ 1.7193e-01, -2.4725e-01, -1.4847e-01, -2.3257e-02, -6.5845e-02,\n",
       "           1.2688e-01, -1.0251e-01,  2.0760e-01],\n",
       "         [-2.5247e-01,  9.2238e-02,  2.4987e-02, -1.6283e-02,  2.7152e-01,\n",
       "          -9.9316e-02,  2.4944e-02,  3.0985e-02],\n",
       "         [ 1.9371e-01, -2.4404e-02,  1.3874e-02,  2.1127e-02, -2.3825e-01,\n",
       "           6.1307e-02,  2.3616e-03, -8.1264e-02],\n",
       "         [-3.7712e-01, -2.4447e-02, -7.8297e-02, -5.4531e-02,  5.1029e-01,\n",
       "          -9.6241e-02, -3.7966e-02,  2.4744e-01]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad6b22-652e-48a8-8076-6d93ed31958a",
   "metadata": {},
   "source": [
    "- The above has made our initial weight to be data driven\n",
    "- Now we need to truncate using trill below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa2a367d-1eb1-4c81-bc84-c8f40c5142c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1861,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.2191,  0.0657,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0849,  0.1212, -0.4896,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0612,  0.0817, -0.0509,  0.0145,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0644, -0.0378,  0.2420,  0.0876, -0.0996,    -inf,    -inf,    -inf],\n",
       "        [-0.1266, -0.1159, -0.0215, -0.0610, -0.0082,  0.1022,    -inf,    -inf],\n",
       "        [ 0.0803,  0.0408,  0.0918,  0.0579, -0.0299, -0.0807,  0.0272,    -inf],\n",
       "        [ 0.1286, -0.0020,  0.3080,  0.1321, -0.1202, -0.1621,  0.0343, -0.0862]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "weight = weight.masked_fill(tril == 0, float('-inf')) # Comment out to obtain bi-directional effect\n",
    "weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf1c1de6-c0e0-424b-854b-081cd239e33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5383, 0.4617, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3453, 0.4243, 0.2304, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2585, 0.2638, 0.2311, 0.2467, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2012, 0.1817, 0.2403, 0.2060, 0.1708, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1522, 0.1538, 0.1690, 0.1625, 0.1713, 0.1913, 0.0000, 0.0000],\n",
       "        [0.1505, 0.1446, 0.1522, 0.1471, 0.1348, 0.1281, 0.1427, 0.0000],\n",
       "        [0.1366, 0.1199, 0.1634, 0.1371, 0.1065, 0.1021, 0.1243, 0.1102]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.softmax(weight, dim=-1)\n",
    "weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d8f662c-260b-4bc1-acef-26b87efbcf68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa99856f-a585-4180-a078-2ff4bf8c7b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x) # we aggregate the values not the exact token, it is also learnable\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29f8dc61-ff33-42b9-b2ab-6e7579794263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_bop3 = weight @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ebf1dba8-8b3d-489d-bd19-71bab3c381e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bop3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f1c84-3f10-4478-83d6-54dfeeae76b2",
   "metadata": {},
   "source": [
    "## Puthing all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "562ca334-70e9-41ca-8b6e-b37fa2539843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        #self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # input of size (batch, latent-space, feature map)\n",
    "        # output of size (batch, latent-space, head size)\n",
    "        \n",
    "        B,T,C = key.shape\n",
    "        \n",
    "        key = self.key(key)   # (B,T,hs)\n",
    "        query = self.query(query) # (B,T,hs)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        weight = query @ key.transpose(-2,-1) # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        \n",
    "        weight = weight * key.shape[-1]**-0.5 # Scale Factor\n",
    "\n",
    "        weight = weight.masked_fill(torch.tril(torch.ones(T,T)) == 0, float('-inf'))# (B, T, T) # Comment out to obtain bi-directional effect\n",
    "        \n",
    "        weight = F.softmax(weight, dim=-1) # (B, T, T)\n",
    "        \n",
    "        weight = self.dropout(weight)\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        value = self.value(value) # (B,T,hs)\n",
    "        out = weight @ value # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846e14d-2452-444c-b112-19165c2fc4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
